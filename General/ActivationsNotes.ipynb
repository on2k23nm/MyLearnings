{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rectified Linear Unit (ReLU) activation**\n",
    "\n",
    "1. **ReLU Activation Function**: ReLU is a widely used activation function in neural networks, especially in convolutional neural networks. It is defined as follows:\n",
    "   $$f(x) = \\max(0, x)$$\n",
    "   This means that if the input $x$ is positive, the output is $x$, and if $x$ is negative, the output is 0. It introduces non-linearity into the network, allowing the network to learn complex mappings from inputs to outputs.\n",
    "\n",
    "2. **The `inplace` Argument**: The `inplace` parameter is a boolean that, when set to `True`, will modify the input directly, without allocating any additional output. It can make the operation slightly more memory efficient since it does not need to save the previous values of the input tensor since they are overwritten with the output.\n",
    "\n",
    "The use of `inplace=True` can be beneficial because it reduces memory usage; however, it must be used with caution. Since the original input is modified, if you need to use the original input tensor later in the code, it won't be available. For instance, this could be problematic in situations where the input tensor is part of a residual connection.\n",
    "\n",
    "Here's how it might look in a PyTorch neural network definition:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        # ReLU activation with inplace set to True\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)  # The ReLU operation will be done in place, modifying x directly\n",
    "        return x\n",
    "```\n",
    "\n",
    "In the code above, the ReLU activation function is applied in place to the output of `self.conv1(x)`, which means the output tensor `x` will be modified directly during the ReLU operation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
